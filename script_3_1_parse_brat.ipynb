{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Step 3.1 Parse PICKLE annotation files__\n",
    "\n",
    "___NOTE: The following needs to be updated___\n",
    "\n",
    "___NOTE: The following needs to be updated___\n",
    "\n",
    "___NOTE: The following needs to be updated___\n",
    "- I move this code from the `kg` repo and the directory structure has changed.\n",
    "- The `0_example` directory is now in:\n",
    "  - `~/github/plantbert/_dev/kg`\n",
    "\n",
    "Goal:\n",
    "- Split train/dev/test\n",
    "- Separate NER and RE annotations\n",
    "- Combine annotation and text files for different docs into one JSON, one for NER, one for RE\n",
    "\n",
    "Notes:\n",
    "- 9/22/23\n",
    "  - Run into issue with tokenization of hyphenated words\n",
    "    - E.g., S-adenosylmethionine synthetase is tokenized into \"S\", \"adenosylmethionine\" and \"synthetase\" which create issue with the indexing.\n",
    "      - See [this](https://stackoverflow.com/questions/58105967/spacy-tokenization-of-hyphenated-words)\n",
    "  - Another tokenization error comes from PMID19413897_abstract.ann\n",
    "    - \"S-adenosylmethionine synthetase, glycine-rich RNA binding protein\"\n",
    "    - This is just bad. So I ___removed___ this form the original file in dev.\n",
    "- 9/14/23\n",
    "  - The pickle corpus and annotation files is from:\n",
    "    - `/mnt/research/ShiuLab/serena_kg/PICKLE_250_abstracts_entities_and_relations_FINAL_05Jul2023`\n",
    "  - Tried to load the PICKLE dataset from huggingface \n",
    "    - The dataset won't load.\n",
    "    - The downloaded train.jsonl does not contain PMID info.\n",
    "    - The dev and test splits are of different sizes\n",
    "    - So do our own split.\n",
    "  - brat to json conversion with [this] works\n",
    "    - But the resulted json cannot be converted to spacy format properly.\n",
    "    - Try to go from brat to IOB, then json, then spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Setup___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "Environment: `torch_spacy`, see:\n",
    "- 0_example/tutorial-ner_bert_spacy.ipynb\n",
    "- 0_example/enviromment.yml\n",
    "- ERROR: No matching distribution found for en-core-web-trf==3.6.1\n",
    "  - I move this offending line to the end of the yml.\n",
    "\n",
    "Environment setup\n",
    "```bash\n",
    "cd /mnt/home/shius/github/kg/0_example\n",
    "module load CUDA/11.8.0\n",
    "conda env create -f environment.yml\n",
    "conda activate torch_spacy\n",
    "python -m spacy download en_core_web_trf\n",
    "```\n",
    "\n",
    "Additional requirements\n",
    "```bash\n",
    "pip install ipywidgets\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, json\n",
    "from pathlib import Path\n",
    "from shutil import copyfile\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import DocBin, Doc\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.char_classes import \\\n",
    "      ALPHA, ALPHA_LOWER, ALPHA_UPPER, CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
    "from spacy.util import compile_infix_regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup directories\n",
    "proj_dir   = Path(\"/mnt/research/compbiol_shiu/kg\")\n",
    "work_dir   = proj_dir / \"1_data_proc\"\n",
    "pickle_dir = work_dir / \"_pickle\"\n",
    "\n",
    "dev_dir    = pickle_dir / \"dev\"\n",
    "test_dir   = pickle_dir / \"test\"\n",
    "train_dir  = pickle_dir / \"train\"\n",
    "\n",
    "dev_dir.mkdir(exist_ok=True, parents=True)\n",
    "test_dir.mkdir(exist_ok=True, parents=True)\n",
    "train_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# brat related\n",
    "brat_config = pickle_dir / \"annotation.conf\"\n",
    "brat2json   = Path.home() / \"bin/bratstandoff-to-json.0.1.0.linux.amd64\"\n",
    "\n",
    "# parameters\n",
    "rand_seed = 20180519\n",
    "\n",
    "[train_r, dev_r, test_r] = [0.6, 0.2, 0.2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Split train/dev/test___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get splits\n",
    "\n",
    "- [Shuffle with seed](https://stackoverflow.com/questions/19306976/python-shuffling-with-a-parameter-to-get-the-same-result\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(file_list, dest_dir):\n",
    "  '''Copy .ann and .txt based on a basename'''\n",
    "  for f in file_list:\n",
    "    f_basename = f.name[:f.name.rfind(\".\")]   # file basename\n",
    "    s_basename = str(f)[:str(f).rfind(\".\")] # source dir + basename\n",
    "\n",
    "    copyfile(s_basename+\".ann\" , dest_dir / (f_basename+\".ann\"))\n",
    "    copyfile(s_basename+\".txt\" , dest_dir / (f_basename+\".txt\"))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_dev_test(pickle_dir, train_r, dev_r, test_r):\n",
    "\n",
    "  # Get list of PMIDs\n",
    "  ann_files = [f for f in pickle_dir.glob(\"*.ann\")]\n",
    "  pmid_list = [x.stem.split(\"_\")[0] for x in ann_files]\n",
    "  print(\"# of PMIDs:\", len(pmid_list))\n",
    "\n",
    "  # Shuffle with seed\n",
    "  random.Random(rand_seed).shuffle(ann_files)\n",
    "\n",
    "  train_files = ann_files[:int(len(ann_files)*train_r)]\n",
    "  dev_files   = ann_files[int(len(ann_files)*train_r):\\\n",
    "                          int(len(ann_files)*(train_r+dev_r))]\n",
    "  test_files  = ann_files[int(len(ann_files)*(train_r+dev_r)):]\n",
    "\n",
    "  print(\"train, dev, test=\", len(train_files), len(dev_files), len(test_files))\n",
    "\n",
    "  copy_files(train_files, train_dir)\n",
    "  copy_files(dev_files, dev_dir)\n",
    "  copy_files(test_files, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of PMIDs: 250\n",
      "train, dev, test= 150 50 50\n"
     ]
    }
   ],
   "source": [
    "split_train_dev_test(pickle_dir, train_r, dev_r, test_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_numbers(dir):\n",
    "  num_ann = len(list(dir.glob(\"*.ann\")))\n",
    "  num_txt = len(list(dir.glob(\"*.txt\")))\n",
    "  print(dir)\n",
    "  print(f\" num ann:{num_ann}, num_txt:{num_txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/research/compbiol_shiu/kg/1_data_proc/_pickle/dev\n",
      " num ann:50, num_txt:50\n",
      "/mnt/research/compbiol_shiu/kg/1_data_proc/_pickle/test\n",
      " num ann:50, num_txt:50\n",
      "/mnt/research/compbiol_shiu/kg/1_data_proc/_pickle/train\n",
      " num ann:150, num_txt:150\n"
     ]
    }
   ],
   "source": [
    "check_file_numbers(dev_dir)\n",
    "check_file_numbers(test_dir)\n",
    "check_file_numbers(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Create JSON files from BRAT annotations___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example doc\n",
    "\n",
    "```python\n",
    "document = \"Gene A is in pathyway 1.\"\n",
    "```\n",
    "\n",
    "BRAT:\n",
    "IOB format\n",
    "```txt\n",
    "T1\\tGENE 0 7\\tGene A\n",
    "T2\\tPATHWAY 14 blah\\pathway 1\n",
    "R2\\tIS_IN Arg1:T1 Arg2:T2\n",
    "```\n",
    "\n",
    "JSON\n",
    "```json\n",
    "[{\"document\":\"Gene A is in pathyway 1.\",\n",
    "  \"tokens\":[{\"text\": \"Gene A\",\n",
    "             \"start\": 0,\n",
    "             \"end\": 5,\n",
    "             \"token_start\": 0,\n",
    "             \"token_end\": 1,\n",
    "             \"entityLabel\": \"GENE\"},\n",
    "            {\"text\": \"pathyway 1\",\n",
    "             \"start\": blah,\n",
    "             \"end\": blah,\n",
    "             \"token_start\": 4,\n",
    "             \"token_end\": 5,\n",
    "             \"entityLabel\": \"PATHWAY\"},],\n",
    "  \"relations\":[\n",
    "     {\"child\":3, \"head\":0, \"relationLabel\":\"IS_IN\"},]\n",
    " }\n",
    "]\n",
    "```\n",
    "\n",
    "Help from:\n",
    "- https://stackoverflow.com/questions/55770365/retrieve-the-span-of-an-entity-from-one-of-its-tokens-in-spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom tokenizer\n",
    "\n",
    "This is to dealt with the \"-\" issue. Solution is from [this post](https://stackoverflow.com/questions/58105967/spacy-tokenization-of-hyphenated-words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(nlp):\n",
    "  infixes = (\n",
    "    LIST_ELLIPSES\n",
    "    + LIST_ICONS\n",
    "    + [\n",
    "        r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
    "        r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n",
    "            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
    "        ),\n",
    "        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
    "        #r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n",
    "        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  infix_re = compile_infix_regex(infixes)\n",
    "\n",
    "  return Tokenizer(nlp.vocab, \n",
    "                   prefix_search=nlp.tokenizer.prefix_search,\n",
    "                   suffix_search=nlp.tokenizer.suffix_search,\n",
    "                   infix_finditer=infix_re.finditer,\n",
    "                   token_match=nlp.tokenizer.token_match,\n",
    "                   rules=nlp.Defaults.tokenizer_exceptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### brat_to_json function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brat_to_json(ann_file, debug=0):\n",
    "  '''Convert brat ann file to a dictionary\n",
    "  Args:\n",
    "    ann_file (Path): brat annotation file\n",
    "    debug (int): print out debug info (1) or not (0)\n",
    "  Returns:\n",
    "    jdict (dict): dictionary of the brat annotation in json format\n",
    "    rdict (dict): dictionary of relations, {relationLabel: count}\n",
    "    err (dict): dictionary of errors\n",
    "  '''\n",
    "\n",
    "  # Read text file\n",
    "  dir      = ann_file.parent\n",
    "  txt_file = dir / f\"{ann_file.stem}.txt\"\n",
    "  txt_list = open(txt_file).readlines()\n",
    "  if len(txt_list) > 1:\n",
    "    print(\"ERR: >1 line in txt file, may have issue with token indexing\")\n",
    "  \n",
    "  document = txt_list[0]\n",
    "  jdict    = {\"document\": document, \"tokens\": [], \"relations\": []}\n",
    "\n",
    "  doc = nlp(document)\n",
    "  \n",
    "  tdict = {} # token dictionary: {Tx: token_start_index}\n",
    "  rdict = {} # {relationLabel: count}\n",
    "  edict = {} # error dictionary\n",
    "\n",
    "  # Read annotations\n",
    "  with open(ann_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    if debug: print(lines)\n",
    "\n",
    "    # Go through lines to deal with tokens first, because tdict needs to be \n",
    "    # built first otherwise cannot figure out token id in relations\n",
    "    for line in lines:\n",
    "      # Read entities\n",
    "      if line[0] == \"T\":\n",
    "        if debug: print(\"entity line:\", [line])\n",
    "        # T_index, entity_info, text\n",
    "        [Tx, entity, text] = line.strip().split(\"\\t\")\n",
    "\n",
    "        # label, start, end\n",
    "        [entityLabel, start, end] = entity.split(\" \")\n",
    "\n",
    "        # Convert to integers\n",
    "        start = int(start)\n",
    "        end   = int(end)\n",
    "\n",
    "        # Get token start/end index\n",
    "        span    = doc.char_span(start, end)\n",
    "\n",
    "        # There are situations where span is None, an example is PMID24519835\n",
    "        # where typo was corrected for tokenization but the text file was not.\n",
    "        # These were ignored.\n",
    "        try:\n",
    "          start_i = span.start\n",
    "          end_i   = span.end - 1 # Note that in IOB this is inclusive\n",
    "        except AttributeError:\n",
    "          if debug: \n",
    "            print(\"ERR: span.start is None\")\n",
    "            print(\"\", ann_file)\n",
    "          edict = {\"span.start_IS_NONE\": [ann_file, Tx, text, start, end]}\n",
    "          break\n",
    "\n",
    "        # Only add to jdict if there is no error\n",
    "        if edict == {}:\n",
    "          tdict[Tx] = start_i\n",
    "          jdict[\"tokens\"].append({\"text\": text, \n",
    "                                  \"start\": start, \n",
    "                                  \"end\": end,\n",
    "                                  \"token_start\": start_i,\n",
    "                                  \"token_end\": end_i, \n",
    "                                  \"entityLabel\": entityLabel})\n",
    "          \n",
    "    # Now deal with relations, only if there is no error from parsing tokens\n",
    "    if edict == {}:\n",
    "      for line in lines:\n",
    "        # Read relations\n",
    "        if line[0] == \"R\":\n",
    "          if debug: print(\"relation line:\", [line])\n",
    "\n",
    "          # relation lable, bits about child, bits about head\n",
    "          [relationLabel, cbit, hbit] = line.strip().split(\"\\t\")[1].split(\" \")\n",
    "\n",
    "          if relationLabel not in rdict:\n",
    "            rdict[relationLabel] = 1\n",
    "          else:\n",
    "            rdict[relationLabel]+= 1\n",
    "\n",
    "          # Tx indices\n",
    "          cTx = cbit.split(\":\")[1]\n",
    "          hTx = hbit.split(\":\")[1]\n",
    "\n",
    "          # Get child and head token indices\n",
    "          child = tdict[cTx]\n",
    "          head  = tdict[hTx]\n",
    "\n",
    "          jdict[\"relations\"].append({\"child\": child, \n",
    "                                    \"head\": head, \n",
    "                                    \"relationLabel\": relationLabel})\n",
    "          \n",
    "    # Return empty dictionary if there is error\n",
    "    if edict != {}:\n",
    "      jdict = {}\n",
    "\n",
    "  return jdict, rdict, edict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'document': 'BACKGROUND: Plants respond to various stress stimuli by activating an enhanced broad-spectrum defensive ability. The development of novel resistance inducers represents an attractive, alternative crop protection strategy. In this regard, hexanoic acid (Hxa, a chemical elicitor) and azelaic acid (Aza, a natural signaling compound) have been proposed as inducers of plant defense, by means of a priming mechanism. Here, we investigated both the mode of action and the complementarity of Aza and Hxa as priming agents in Nicotiana tabacum cells in support of enhanced defense. RESULTS: Metabolomic analyses identified signatory biomarkers involved in the establishment of a pre-conditioned state following Aza and Hxa treatment. Both inducers affected the metabolomes in a similar manner and generated common biomarkers: caffeoylputrescine glycoside, cis-5-caffeoylquinic acid, feruloylglycoside, feruloyl-3-methoxytyramine glycoside and feruloyl-3-methoxytyramine conjugate. Subsequently, quantitative real time-PCR was used to investigate the expression of inducible defense response genes: phenylalanine ammonia lyase, hydroxycinnamoyl CoA quinate transferase and hydroxycinnamoyl transferase to monitor activation of the early phenylpropanoid pathway and chlorogenic acids metabolism, while ethylene response element-binding protein, small sar1 GTPase, heat shock protein 90, RAR1, SGT1, non-expressor of PR genes 1 and thioredoxin were analyzed to report on signal transduction events. Pathogenesis-related protein 1a and defensin were quantified to investigate the activation of defenses regulated by salicylic acid and jasmonic acid respectively. The qPCR results revealed differential expression kinetics and, in general (except for NPR1, Thionin and PR1a), the relative gene expression ratios observed in the Hxa-treated cells were significantly greater than the expression observed in the cells treated with Aza. CONCLUSIONS: The results indicate that Aza and Hxa have a similar priming effect through activation of genes involved in the establishment of systemic acquired resistance, associated with enhanced synthesis of hydroxycinnamic acids and related conjugates.',\n",
       "  'tokens': [{'text': 'hexanoic acid',\n",
       "    'start': 238,\n",
       "    'end': 251,\n",
       "    'token_start': 35,\n",
       "    'token_end': 36,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'Hxa',\n",
       "    'start': 253,\n",
       "    'end': 256,\n",
       "    'token_start': 38,\n",
       "    'token_end': 38,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'azelaic acid',\n",
       "    'start': 283,\n",
       "    'end': 295,\n",
       "    'token_start': 45,\n",
       "    'token_end': 46,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'Aza',\n",
       "    'start': 297,\n",
       "    'end': 300,\n",
       "    'token_start': 48,\n",
       "    'token_end': 48,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'Aza',\n",
       "    'start': 487,\n",
       "    'end': 490,\n",
       "    'token_start': 84,\n",
       "    'token_end': 84,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'Hxa',\n",
       "    'start': 495,\n",
       "    'end': 498,\n",
       "    'token_start': 86,\n",
       "    'token_end': 86,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'Nicotiana tabacum cells',\n",
       "    'start': 520,\n",
       "    'end': 543,\n",
       "    'token_start': 91,\n",
       "    'token_end': 93,\n",
       "    'entityLabel': 'Cell'},\n",
       "   {'text': 'Aza',\n",
       "    'start': 705,\n",
       "    'end': 708,\n",
       "    'token_start': 116,\n",
       "    'token_end': 116,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'Hxa',\n",
       "    'start': 713,\n",
       "    'end': 716,\n",
       "    'token_start': 118,\n",
       "    'token_end': 118,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'caffeoylputrescine glycoside',\n",
       "    'start': 820,\n",
       "    'end': 848,\n",
       "    'token_start': 135,\n",
       "    'token_end': 136,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'cis-5-caffeoylquinic acid',\n",
       "    'start': 850,\n",
       "    'end': 875,\n",
       "    'token_start': 138,\n",
       "    'token_end': 139,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'feruloylglycoside',\n",
       "    'start': 877,\n",
       "    'end': 894,\n",
       "    'token_start': 141,\n",
       "    'token_end': 141,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'feruloyl-3-methoxytyramine glycoside',\n",
       "    'start': 896,\n",
       "    'end': 932,\n",
       "    'token_start': 143,\n",
       "    'token_end': 144,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'feruloyl-3-methoxytyramine conjugate',\n",
       "    'start': 937,\n",
       "    'end': 973,\n",
       "    'token_start': 146,\n",
       "    'token_end': 147,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'phenylalanine ammonia lyase',\n",
       "    'start': 1092,\n",
       "    'end': 1119,\n",
       "    'token_start': 166,\n",
       "    'token_end': 168,\n",
       "    'entityLabel': 'DNA'},\n",
       "   {'text': 'hydroxycinnamoyl CoA quinate transferase',\n",
       "    'start': 1121,\n",
       "    'end': 1161,\n",
       "    'token_start': 170,\n",
       "    'token_end': 173,\n",
       "    'entityLabel': 'DNA'},\n",
       "   {'text': 'hydroxycinnamoyl transferase',\n",
       "    'start': 1166,\n",
       "    'end': 1194,\n",
       "    'token_start': 175,\n",
       "    'token_end': 176,\n",
       "    'entityLabel': 'DNA'},\n",
       "   {'text': 'early phenylpropanoid pathway',\n",
       "    'start': 1224,\n",
       "    'end': 1253,\n",
       "    'token_start': 182,\n",
       "    'token_end': 184,\n",
       "    'entityLabel': 'Biochemical_pathway'},\n",
       "   {'text': 'chlorogenic acids metabolism',\n",
       "    'start': 1258,\n",
       "    'end': 1286,\n",
       "    'token_start': 186,\n",
       "    'token_end': 188,\n",
       "    'entityLabel': 'Biochemical_process'},\n",
       "   {'text': 'ethylene response element-binding protein',\n",
       "    'start': 1294,\n",
       "    'end': 1335,\n",
       "    'token_start': 191,\n",
       "    'token_end': 194,\n",
       "    'entityLabel': 'DNA'},\n",
       "   {'text': 'small sar1 GTPase',\n",
       "    'start': 1337,\n",
       "    'end': 1354,\n",
       "    'token_start': 196,\n",
       "    'token_end': 198,\n",
       "    'entityLabel': 'DNA'},\n",
       "   {'text': 'heat shock protein 90',\n",
       "    'start': 1356,\n",
       "    'end': 1377,\n",
       "    'token_start': 200,\n",
       "    'token_end': 203,\n",
       "    'entityLabel': 'DNA'},\n",
       "   {'text': 'Pathogenesis-related protein 1a',\n",
       "    'start': 1490,\n",
       "    'end': 1521,\n",
       "    'token_start': 225,\n",
       "    'token_end': 227,\n",
       "    'entityLabel': 'Protein'},\n",
       "   {'text': 'defensin',\n",
       "    'start': 1526,\n",
       "    'end': 1534,\n",
       "    'token_start': 229,\n",
       "    'token_end': 229,\n",
       "    'entityLabel': 'Protein'},\n",
       "   {'text': 'salicylic acid',\n",
       "    'start': 1606,\n",
       "    'end': 1620,\n",
       "    'token_start': 240,\n",
       "    'token_end': 241,\n",
       "    'entityLabel': 'Plant_hormone'},\n",
       "   {'text': 'jasmonic acid',\n",
       "    'start': 1625,\n",
       "    'end': 1638,\n",
       "    'token_start': 243,\n",
       "    'token_end': 244,\n",
       "    'entityLabel': 'Plant_hormone'},\n",
       "   {'text': 'NPR1',\n",
       "    'start': 1740,\n",
       "    'end': 1744,\n",
       "    'token_start': 261,\n",
       "    'token_end': 261,\n",
       "    'entityLabel': 'DNA'},\n",
       "   {'text': 'Thionin',\n",
       "    'start': 1746,\n",
       "    'end': 1753,\n",
       "    'token_start': 263,\n",
       "    'token_end': 263,\n",
       "    'entityLabel': 'DNA'},\n",
       "   {'text': 'PR1a',\n",
       "    'start': 1758,\n",
       "    'end': 1762,\n",
       "    'token_start': 265,\n",
       "    'token_end': 265,\n",
       "    'entityLabel': 'DNA'},\n",
       "   {'text': 'Aza',\n",
       "    'start': 1917,\n",
       "    'end': 1920,\n",
       "    'token_start': 290,\n",
       "    'token_end': 290,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'Aza',\n",
       "    'start': 1961,\n",
       "    'end': 1964,\n",
       "    'token_start': 298,\n",
       "    'token_end': 298,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'Hxa',\n",
       "    'start': 1969,\n",
       "    'end': 1972,\n",
       "    'token_start': 300,\n",
       "    'token_end': 300,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'systemic acquired resistance',\n",
       "    'start': 2064,\n",
       "    'end': 2092,\n",
       "    'token_start': 315,\n",
       "    'token_end': 317,\n",
       "    'entityLabel': 'Biochemical_process'},\n",
       "   {'text': 'hydroxycinnamic acids',\n",
       "    'start': 2132,\n",
       "    'end': 2153,\n",
       "    'token_start': 324,\n",
       "    'token_end': 325,\n",
       "    'entityLabel': 'Organic_compound_other'},\n",
       "   {'text': 'RAR1',\n",
       "    'start': 1379,\n",
       "    'end': 1383,\n",
       "    'token_start': 205,\n",
       "    'token_end': 205,\n",
       "    'entityLabel': 'DNA'},\n",
       "   {'text': 'SGT1',\n",
       "    'start': 1385,\n",
       "    'end': 1389,\n",
       "    'token_start': 207,\n",
       "    'token_end': 207,\n",
       "    'entityLabel': 'DNA'},\n",
       "   {'text': 'non-expressor of PR genes 1',\n",
       "    'start': 1391,\n",
       "    'end': 1418,\n",
       "    'token_start': 209,\n",
       "    'token_end': 213,\n",
       "    'entityLabel': 'DNA'},\n",
       "   {'text': 'thioredoxin',\n",
       "    'start': 1423,\n",
       "    'end': 1434,\n",
       "    'token_start': 215,\n",
       "    'token_end': 215,\n",
       "    'entityLabel': 'DNA'}],\n",
       "  'relations': [{'child': 298, 'head': 315, 'relationLabel': 'interacts'},\n",
       "   {'child': 300, 'head': 315, 'relationLabel': 'interacts'},\n",
       "   {'child': 298, 'head': 324, 'relationLabel': 'activates'},\n",
       "   {'child': 300, 'head': 324, 'relationLabel': 'activates'}]},\n",
       " {'interacts': 2, 'activates': 2},\n",
       " {})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing brat_to_dict\n",
    "#ann = pickle_dir / \"PMID24519835_abstract.ann\" # this one has error\n",
    "ann = pickle_dir / \"PMID29187153_abstract.ann\"\n",
    "j_dict, rdict, edict = brat_to_json(ann)\n",
    "j_dict, rdict, edict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all brat files to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_brat_to_json(dir, out_path):\n",
    "  '''Convert all brat files in a directory to json\n",
    "  Args:\n",
    "    dir (Path): directory of brat files\n",
    "    out_path (Path): output json file\n",
    "  Returns:\n",
    "    relations (dict): {relationLabel: count}\n",
    "  '''\n",
    "  \n",
    "  j_list    = [] # list of dictionaries\n",
    "  relations = {} # relation counts\n",
    "  c_err     = 0  # count errors\n",
    "\n",
    "  f_list    = [] # list of qualified files\n",
    "\n",
    "  print(f\"{dir.stem}, errors:\")\n",
    "  for ann_file in dir.glob(\"*.ann\"):\n",
    "    jdict, rdict, edict = brat_to_json(ann_file)\n",
    "\n",
    "    # only add to list if there is no error\n",
    "    if edict == {}:\n",
    "      j_list.append(jdict)\n",
    "      f_list.append(ann_file)\n",
    "    else:\n",
    "      c_err += 1\n",
    "\n",
    "    for e in edict:\n",
    "      print(\"\", edict[e][0].stem)\n",
    "\n",
    "    # populate relations\n",
    "    for r in rdict:\n",
    "      if r not in relations:\n",
    "        relations[r] = rdict[r]\n",
    "      else:\n",
    "        relations[r] += rdict[r]\n",
    "\n",
    "  # generate output\n",
    "  with open(out_path, \"w\") as f:\n",
    "    json.dump(j_list, f)\n",
    "\n",
    "  print(f\" number errors:{c_err}\")\n",
    "\n",
    "  return relations, f_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev, errors:\n",
      " PMID24519835_abstract\n",
      " PMID23297052_abstract\n",
      " number errors:2\n",
      "test, errors:\n",
      " PMID27927228_abstract\n",
      " PMID18450451_abstract\n",
      " PMID27540390_abstract\n",
      " PMID29062306_abstract\n",
      " PMID16664167_abstract\n",
      " PMID16169957_abstract\n",
      " PMID30272908_abstract\n",
      " number errors:7\n",
      "train, errors:\n",
      " PMID24515663_abstract\n",
      " PMID24997625_abstract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PMID33357541_abstract\n",
      " PMID17644730_abstract\n",
      " PMID30727640_abstract\n",
      " PMID24720904_abstract\n",
      " PMID20339157_abstract\n",
      " PMID18992204_abstract\n",
      " number errors:8\n"
     ]
    }
   ],
   "source": [
    "json_dev = work_dir / \"dev.json\"\n",
    "json_test = work_dir / \"test.json\"\n",
    "json_train = work_dir / \"train.json\"\n",
    "\n",
    "rel_dev, flist_dev     = convert_all_brat_to_json(dev_dir, json_dev)\n",
    "rel_test, flist_test   = convert_all_brat_to_json(test_dir, json_test)\n",
    "rel_train, flist_train = convert_all_brat_to_json(train_dir, json_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'interacts': 122,\n",
       "  'is-in': 81,\n",
       "  'activates': 83,\n",
       "  'inhibits': 56,\n",
       "  'produces': 28},\n",
       " {'activates': 74,\n",
       "  'is-in': 80,\n",
       "  'interacts': 114,\n",
       "  'inhibits': 60,\n",
       "  'produces': 28},\n",
       " {'interacts': 389,\n",
       "  'inhibits': 234,\n",
       "  'activates': 316,\n",
       "  'is-in': 316,\n",
       "  'produces': 67})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_dev, rel_test, rel_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Convret JSON to spacy___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function\n",
    "\n",
    "Modify code from [this](https://github.com/walidamamou/relation_extraction_transformer/blob/main/binary_converter.py), which is originally from [here](https://github.com/explosion/projects/blob/v3/tutorials/rel_component/scripts/parse_data.py/).\n",
    "- The code from walidamamou has unnecessary parts. \n",
    "  - The original code use the ending number in article ids to decide if the doc is for dev (end with 4), test (end with 3), or train (everthing else). But walidamamou's code is just about train.\n",
    "  - The `ids`, `vocab` variables are not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_spacy(json_loc, spacy_loc, MAP_LABELS, flist):\n",
    "  \"\"\"Creating the corpus from the Prodigy annotations\n",
    "  Args:\n",
    "    json_loc (Path): Location of the JSON file\n",
    "    spacy_loc (Path): Location of the spacy output file\n",
    "  \"\"\"\n",
    "  \n",
    "  Doc.set_extension(\"rel\", default={},force=True)\n",
    "  vocab = Vocab()\n",
    "  docs  = []\n",
    "\n",
    "  with open(json_loc, encoding=\"utf8\") as jsonfile:\n",
    "    jlist = json.load(jsonfile) # list of dictionaries\n",
    "    for idx, jdict in enumerate(jlist):\n",
    "      span_starts = set()\n",
    "      neg = 0\n",
    "      pos = 0\n",
    "      err = 0\n",
    "      \n",
    "      # Parse the tokens\n",
    "      tokens = nlp(jdict[\"document\"])  \n",
    "      spaces = []\n",
    "      spaces = [True if tok.whitespace_ else False for tok in tokens]\n",
    "      words  = [t.text for t in tokens]\n",
    "      doc    = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "      # Parse entities\n",
    "      spans    = jdict[\"tokens\"]\n",
    "      entities = []\n",
    "      span_end_to_start = {}\n",
    "      for span in spans:\n",
    "        entity = doc.char_span(span[\"start\"], span[\"end\"], \n",
    "                               label=span[\"entityLabel\"])\n",
    "        span_end_to_start[span[\"token_start\"]] = span[\"token_start\"]\n",
    "        entities.append(entity)\n",
    "        span_starts.add(span[\"token_start\"])\n",
    "\n",
    "      try:\n",
    "        doc.ents = entities\n",
    "      except ValueError:\n",
    "        err = 1\n",
    "        # This error is originally found in PMID19413897_abstract.ann where\n",
    "        # there is an enity spanning \",\" which does not make sense. This was\n",
    "        # removed from the annotation.\n",
    "        print(\"ERR: ValueError at doc index=\", idx)\n",
    "        print(\"\", flist[idx])\n",
    "        print(\"\", entities)\n",
    "        print(\"\", tokens[335 ])\n",
    "        print(\"\", tokens[335-5:335+5])\n",
    "        break\n",
    "      \n",
    "\n",
    "      # Parse the relations\n",
    "      # Creat a dict with all possible relations\n",
    "      rels = {}\n",
    "      for x1 in span_starts:\n",
    "        for x2 in span_starts:\n",
    "          rels[(x1, x2)] = {}\n",
    "\n",
    "      relations = jdict[\"relations\"]\n",
    "      for relation in relations:\n",
    "        # the 'head' and 'child' annotations refer to the end token in the span\n",
    "        # but we want the first token\n",
    "        start = span_end_to_start[relation[\"head\"]]\n",
    "        end = span_end_to_start[relation[\"child\"]]\n",
    "        label = relation[\"relationLabel\"]\n",
    "\n",
    "        if label not in rels[(start, end)]:\n",
    "          rels[(start, end)][label] = 1.0\n",
    "          pos += 1\n",
    "\n",
    "      # fill in zero's where the data is missing\n",
    "      for x1 in span_starts:\n",
    "        for x2 in span_starts:\n",
    "          for label in MAP_LABELS.values():\n",
    "            if label not in rels[(x1, x2)]:\n",
    "              neg += 1\n",
    "              rels[(x1, x2)][label] = 0.0\n",
    "\n",
    "              #print(rels[(x1, x2)])\n",
    "      doc._.rel = rels\n",
    "      \n",
    "      docs.append(doc)\n",
    "\n",
    "  # Save docs to a spacy format file\n",
    "  docbin = DocBin(docs=docs, store_user_data=True)\n",
    "  docbin.to_disk(spacy_loc)\n",
    "\n",
    "  return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank Tokenizer with just the English vocab\n",
    "#nlp = spacy.blank(\"en\")\n",
    "# commented out since nlp is created earlier with English().\n",
    "\n",
    "spacy_train = work_dir / \"train.spacy\"\n",
    "spacy_dev   = work_dir / \"dev.spacy\"\n",
    "spacy_test  = work_dir / \"test.spacy\"\n",
    "\n",
    "MAP_LABELS = {label:label for label in rel_train}\n",
    "\n",
    "train_docs = json_to_spacy(json_train, spacy_train, MAP_LABELS, flist_train)\n",
    "dev_docs   = json_to_spacy(json_dev, spacy_dev, MAP_LABELS, flist_dev)\n",
    "test_docs  = json_to_spacy(json_test, spacy_test, MAP_LABELS, flist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 48, 43)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_docs), len(dev_docs), len(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(work_dir)\n",
    "!chmod 771 -R *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Testing___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pickle dataset from Huggingface\n",
    "\n",
    "To get train/dev/test info from [Huggingface](https://huggingface.co/datasets/slotreck/pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "from datasets import load_dataset\n",
    "\n",
    "pk_hf_dir  = work_dir / \"pickle_huggingface\"\n",
    "pk_hf_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This leads to JSONDecodeError\n",
    "datasets = load_dataset(\"slotreck/pickle\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the files directly\n",
    "url_hf     = \"https://huggingface.co/datasets/slotreck/pickle/blob/main/\"\n",
    "file_dev   = \"dev.jsonl\"\n",
    "file_test  = \"test.jsonl\"\n",
    "file_train = \"train.jsonl\"\n",
    "\n",
    "wget.download(url_hf + file_dev,   str(pk_hf_dir / file_dev))\n",
    "wget.download(url_hf + file_test,  str(pk_hf_dir / file_test))\n",
    "wget.download(url_hf + file_train, str(pk_hf_dir / file_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(jsonl_file):\n",
    "  '''Get PMID IDs from a jsonl file'''\n",
    "\n",
    "  # JSON file lines\n",
    "  jlines = open(jsonl_file).readlines()\n",
    "\n",
    "  tag   = \"PMID\"   # target tag\n",
    "  pmids = []       # PMID ID list\n",
    "  for jline in jlines:\n",
    "    if jline.find(tag) != -1:\n",
    "      jline = jline[jline.find(tag):]\n",
    "      pmid  = jline.split(\"_abstract\")[0]\n",
    "      pmids.append(pmid)\n",
    "\n",
    "  print(f\" # PMID IDs: {len(pmids)}\")\n",
    "\n",
    "  return pmids\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_dev   = get_ids(pk_hf_dir / file_dev)\n",
    "pmid_test  = get_ids(pk_hf_dir / file_test)\n",
    "pmid_train = get_ids(pk_hf_dir / file_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate NER and RE annotations\n",
    "\n",
    "In the brat annotation files, entity and relation annotations are in one file with different prefix T (entity) and R (relation). The code below is to separate them into different files before generating JSON for spacy.\n",
    "- This is not necessary as spacy can choose which part to use.\n",
    "- Deprecated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_ner_re_ann(dir):\n",
    "  ann_list = dir.glob(\"*.ann\")\n",
    "\n",
    "  ner_out = dir / \"NER\"\n",
    "  re_out  = dir / \"RE\"\n",
    "  ner_out.mkdir(exist_ok=True, parents=True)\n",
    "  re_out.mkdir(exist_ok=True, parents=True)\n",
    "  \n",
    "  for ann in ann_list:\n",
    "    ann_name   = ann.name\n",
    "    with open(ann, \"r\") as f:\n",
    "      ann_lines = f.readlines()\n",
    "    ner_lines = [x for x in ann_lines if x[0]==\"T\"]\n",
    "    re_lines  = [x for x in ann_lines if x[0]==\"R\"]\n",
    "    with open(ner_out / (f\"{ann_name}\"), \"w\") as f:\n",
    "      f.writelines(ner_lines)\n",
    "    with open(re_out / (f\"{ann_name}\"), \"w\") as f:\n",
    "      f.writelines(re_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_ner_re_ann(dev_dir)\n",
    "sep_ner_re_ann(test_dir)\n",
    "sep_ner_re_ann(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert brat to json\n",
    "\n",
    "Make use of [brat-standoff-to-json](https://github.com/astutic/brat-standoff-to-json)\n",
    "- This creates a json format that Acharya can read. BUT!!!\n",
    "- This does not led to json that can be used by spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate shell scripts\n",
    "\n",
    "```bash\n",
    "brat_standoff-to-json \\\n",
    " -a \"ann1,ann2,...\"\n",
    " -t \"txt1,txt2,...\"\n",
    " -c config_file \n",
    " -o output_file\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shell_script(dir, subset, task):\n",
    "  '''Generate shell script to run task (NER or RE)'''\n",
    "\n",
    "  # dir with annotation files for the task\n",
    "  task_out = dir / task\n",
    "\n",
    "  txt_list = [t for t in dir.glob(\"*.txt\")]\n",
    "  ann_list = []\n",
    "  for t in txt_list:\n",
    "    t_stem   = t.stem\n",
    "    ann_list.append(f\"{str(task_out)}/{t.stem}.ann\")\n",
    "\n",
    "  ann_str  = \",\".join([str(x) for x in ann_list])\n",
    "  txt_str  = \",\".join([str(x) for x in txt_list])\n",
    "\n",
    "  script_file = dir / f\"run_{task}.sh\"\n",
    "  json_out    = dir / f\"{subset}_{task}.json\"\n",
    "  with open(script_file, \"w\") as f:\n",
    "    f.write(f\"{brat2json} -c {brat_config} -o {json_out} \")\n",
    "    f.write(f' -a {ann_str} -t {txt_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_shell_script(dev_dir, \"dev\", \"NER\")\n",
    "generate_shell_script(dev_dir, \"dev\", \"RE\")\n",
    "generate_shell_script(test_dir, \"test\", \"NER\")\n",
    "generate_shell_script(test_dir, \"test\", \"RE\")\n",
    "generate_shell_script(train_dir, \"train\", \"NER\")\n",
    "generate_shell_script(train_dir, \"train\", \"RE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run shell scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dev_dir)\n",
    "!chmod 771 -R *.sh\n",
    "!./run_NER.sh\n",
    "!./run_RE.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(train_dir)\n",
    "!chmod 771 -R *.sh\n",
    "!./run_NER.sh\n",
    "!./run_RE.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(test_dir)\n",
    "!chmod 771 -R *.sh\n",
    "!./run_NER.sh\n",
    "!./run_RE.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert brat to IOB\n",
    "\n",
    "Original plan to do brat-json-spacy does not work.\n",
    "- Try brat-iob-json-spacy\n",
    "- See [this repo](https://github.com/PL97/Brat2BIO)\n",
    "\n",
    "Install standord core nlp\n",
    "```bash\n",
    "cd ~/bin\n",
    "wget https://nlp.stanford.edu/software/stanford-corenlp-4.5.5.zip\n",
    "unzip stanford-corenlp-4.5.5.zip\n",
    "cd stanford-corenlp-4.5.5\n",
    "```\n",
    "\n",
    "Clone \n",
    "```\n",
    "cd ~/github\n",
    "git clone https://github.com/PL97/Brat2BIO.git\n",
    "cd Brad2BIO\n",
    "chmod +x convert.sh\n",
    "./convert.sh sample output\n",
    "```\n",
    "\n",
    "Include env variables in .bashrc\n",
    "\n",
    "This thing is a beast and, due to env var issue, did not run properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test spacy tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Reducing phenylpropanoid biosynthesis\"\n",
    "doc = nlp(txt)\n",
    "len(txt), len(doc), [idx for idx, chr in enumerate(txt) if chr==\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "  print(token.text, token.i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = doc.char_span(9, 37)\n",
    "s.text, s.start, s.end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
