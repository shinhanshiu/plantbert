{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Plant science word frequency__\n",
    "\n",
    "Goal\n",
    "- Evaluate plant science corpus to see how best to tokenize the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast\n",
    "from tokenizers import BertWordPieceTokenizer, ByteLevelBPETokenizer, \\\n",
    "                       CharBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir    = Path.home() / \"projects/plantbert\"\n",
    "corpus_file = work_dir / \"corpus_with_topics.tsv.gz\"\n",
    "train_file  = work_dir / \"train.txt\"\n",
    "\n",
    "tokenizer_dir = work_dir / \"tokenizers\"\n",
    "tokenizer_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file.is_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Corpus word frequencies___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Index_1385417</th>\n",
       "      <th>PMID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Initial filter qualifier</th>\n",
       "      <th>Corpus</th>\n",
       "      <th>reg_article</th>\n",
       "      <th>Text classification score</th>\n",
       "      <th>Preprocessed corpus</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>1975-12-11</td>\n",
       "      <td>Biochimica et biophysica acta</td>\n",
       "      <td>Identification of the 120 mus phase in the dec...</td>\n",
       "      <td>After a 500 mus laser flash a 120 mus phase in...</td>\n",
       "      <td>spinach</td>\n",
       "      <td>Identification of the 120 mus phase in the dec...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716394</td>\n",
       "      <td>identification 120 mus phase decay delayed flu...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>1975-11-20</td>\n",
       "      <td>Biochimica et biophysica acta</td>\n",
       "      <td>Cholinesterases from plant tissues. VI. Prelim...</td>\n",
       "      <td>Enzymes capable of hydrolyzing esters of thioc...</td>\n",
       "      <td>plant</td>\n",
       "      <td>Cholinesterases from plant tissues. VI. Prelim...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.894874</td>\n",
       "      <td>cholinesterases plant tissues . vi . prelimina...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Index_1385417  PMID        Date                        Journal  \\\n",
       "0           0              3    61  1975-12-11  Biochimica et biophysica acta   \n",
       "1           1              4    67  1975-11-20  Biochimica et biophysica acta   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Identification of the 120 mus phase in the dec...   \n",
       "1  Cholinesterases from plant tissues. VI. Prelim...   \n",
       "\n",
       "                                            Abstract Initial filter qualifier  \\\n",
       "0  After a 500 mus laser flash a 120 mus phase in...                  spinach   \n",
       "1  Enzymes capable of hydrolyzing esters of thioc...                    plant   \n",
       "\n",
       "                                              Corpus  reg_article  \\\n",
       "0  Identification of the 120 mus phase in the dec...            1   \n",
       "1  Cholinesterases from plant tissues. VI. Prelim...            1   \n",
       "\n",
       "   Text classification score  \\\n",
       "0                   0.716394   \n",
       "1                   0.894874   \n",
       "\n",
       "                                 Preprocessed corpus  Topic  \n",
       "0  identification 120 mus phase decay delayed flu...     52  \n",
       "1  cholinesterases plant tissues . vi . prelimina...     48  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv(corpus_file, sep=\"\\t\", compression=\"gzip\")\n",
    "corpus.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421307"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = corpus[\"Corpus\"].tolist()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine frequenies of words in corpus\n",
    "\n",
    "See:\n",
    "- [Word freq](https://www.educative.io/answers/text-summarization-in-spacy-and-nltk) \n",
    "- [Multiprocessing pool](https://superfastpython.com/multiprocessing-pool-for-loop/)\n",
    "- [Fill up a dictionary in parallel with multiprocessing](https://python-forum.io/thread-8587.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shius/miniconda3/envs/torch_spacy/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421307/421307 [04:09<00:00, 1691.19it/s]\n"
     ]
    }
   ],
   "source": [
    "word_freq = {}\n",
    "for doc in tqdm(docs):\n",
    "  doc = nlp(doc)\n",
    "  for token in doc:\n",
    "    if token.text not in STOP_WORDS and token.text not in punctuation:\n",
    "      if token.text not in word_freq:\n",
    "        word_freq[token.text] = 1\n",
    "      else:\n",
    "        word_freq[token.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "908607"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('284c51', 1),\n",
       " ('14,200+/-900', 1),\n",
       " ('trypsin[EC', 1),\n",
       " ('K+/2e', 1),\n",
       " ('acrA.', 1),\n",
       " ('gal+', 1),\n",
       " ('20mum', 1),\n",
       " ('inhilating', 1),\n",
       " ('wasnot', 1),\n",
       " ('Sepharose-6B.', 1),\n",
       " ('Proteindisulphide', 1),\n",
       " ('nott', 1),\n",
       " ('trypsinmodified', 1),\n",
       " ('pH-7.6', 1),\n",
       " ('O2(18', 1),\n",
       " ('Stoichacis', 1),\n",
       " ('E.C.1.8.5.1', 1),\n",
       " ('I-21A', 1),\n",
       " ('feremented', 1),\n",
       " ('Iwaasa', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.freecodecamp.org/news/sort-dictionary-by-value-in-python/\n",
    "\n",
    "word_freq_sorted = sorted(word_freq.items(), key=lambda x:x[1])\n",
    "word_freq_sorted[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13530, 11186, 28007, 23572, 33303)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq[\"SA\"], word_freq[\"JA\"], word_freq[\"auxin\"], word_freq[\"ethylene\"], word_freq[\"ABA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1339"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq[\"FLC\"], word_freq[\"FLC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Train different tokenizer___\n",
    "\n",
    "Tokenizers to try\n",
    "- BertWordPieceTokenizer\n",
    "- ByteLevelBPETokenizer\n",
    "- CharBPETokenizer\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"]\n",
    "\n",
    "# training the tokenizer on the training set\n",
    "files = [str(train_file)]\n",
    "\n",
    "# 30,522 vocab is BERT's default vocab size\n",
    "vocab_size = 30_522\n",
    "\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(tokenizer_type, train_file, vocab_size, max_length):\n",
    "\n",
    "  tokenizer = None\n",
    "  if tokenizer_type == \"bwp\":\n",
    "    tokenizer = BertWordPieceTokenizer()\n",
    "  elif tokenizer_type == \"blbpe\":\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "  elif tokenizer_type == \"chbpe\":\n",
    "    tokenizer = CharBPETokenizer()\n",
    "  else:\n",
    "    print(\"Unknown tokenizer type:\", tokenizer_type)\n",
    "    return 0\n",
    "\n",
    "  # train the tokenizer\n",
    "  tokenizer.train(files=str(train_file), vocab_size=vocab_size, \n",
    "                      special_tokens=special_tokens)\n",
    "\n",
    "  # enable truncation up to the maximum 512 tokens\n",
    "  tokenizer.enable_truncation(max_length=max_length)\n",
    "\n",
    "  # save the tokenizer\n",
    "  save_dir = tokenizer_dir / tokenizer_type\n",
    "  save_dir.mkdir(parents=True, exist_ok=True)\n",
    "  tokenizer.save_model(str(save_dir))\n",
    "\n",
    "  # when the tokenizer is trained and configured, load it as BertTokenizerFast\n",
    "  #btz_tokenizer = BertTokenizerFast.from_pretrained(save_dir)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BertWordPieceTokenizer\n",
    "train_tokenizer(\"bwp\", train_file, vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ByteLevelBPETokenizer\n",
    "train_tokenizer(\"blbpe\", train_file, vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CharBPETokenizer\n",
    "train_tokenizer(\"chbpe\", train_file, vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
